package conduct.kernels;

import com.maxeler.maxcompiler.v2.kernelcompiler.Kernel;
import com.maxeler.maxcompiler.v2.kernelcompiler.KernelParameters;
import com.maxeler.maxcompiler.v2.kernelcompiler.SMIO;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEVar;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.base.DFEType;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEArray;
import com.maxeler.maxcompiler.v2.kernelcompiler.types.composite.DFEArrayType;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.IO.*;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.core.Count.Counter;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.memory.Memory;
import com.maxeler.maxcompiler.v2.kernelcompiler.stdlib.Reductions;
import conduct.GC;

public class NbrPropRdr extends Kernel {

  private void printArray(DFEVar[] ar, int size) {
    for (int i=0; i < size; i++) { 
      debug.printf("%d ", ar[size-1-i]); 
    }
  }

  public NbrPropRdr(KernelParameters parameters) {
    super(parameters);
    DFEVar cnt = control.count.simpleCounter(64);
    // Scalar Inputs
    DFEVar nodeAddr = io.scalarInput("nodeAddr", dfeUInt(GC.scalWidth));
    DFEVar repAddr  = io.scalarInput("repAddr" , dfeUInt(GC.scalWidth));
    DFEVar uDVal    = io.scalarInput("uDVal"   , dfeUInt(GC.scalWidth));

    // Inputs from DRAM:  node lists
    int arSize = GC.NODESPERPKT;
    DFEArrayType<DFEVar> arrayType = 
      new DFEArrayType<DFEVar>(dfeUInt(GC.NODEWIDTH), arSize);

    NonBlockingInput<DFEArray<DFEVar>> nodeArray_In = 
      io.nonBlockingInput("nodeArray", arrayType, constant.var(true), 1,
                          DelimiterMode.FRAME_LENGTH, 0, NonBlockingMode.NO_TRICKLING);
    DFEArray<DFEVar> nodeArray = nodeArray_In.data;
    DFEVar nodeArrayCtrl       = nodeArray_In.valid;

    // elem is node being accessed
    NonBlockingInput<DFEVar> elem_Input =
      io.nonBlockingInput("elem", dfeUInt(32), constant.var(true), 1, 
                          DelimiterMode.FRAME_LENGTH, 0, NonBlockingMode.NO_TRICKLING);
    DFEVar elem_In  = elem_Input.data;
    DFEVar elemCtrl = elem_Input.valid;
    DFEVar elemIdx_In = elem_In.slice(0, GC.nPBBits).cast(dfeUInt(GC.nPBBits));

    // Feedback stall input
    NonBlockingInput<DFEVar> outputStall_In =
      io.nonBlockingInput("outputStall", dfeBool(), constant.var(true), 1, 
                          DelimiterMode.FRAME_LENGTH, 0, NonBlockingMode.NO_TRICKLING);
    DFEVar outputStall = Reductions.streamHold(outputStall_In.data, outputStall_In.valid,
                                               dfeBool().encodeConstant(false));

    // Queue Reader SM
    // - for each elemIdx, tracks how many packets need to be consumed before
    //   incrementing the RAM read pointer
    // - flexible approach is needed to account for overflow (receive 2 bursts)
    int buf = 2;
    int pktCntBits = GC.pktsPBBits+buf;  // Allow space for multiple bursts
    DFEType pktCntType = dfeUInt(pktCntBits);
    int lastNodeInBrstIdx = GC.NODESPERBRST-1;
    DFEVar ovflow_In = elemIdx_In.eq(lastNodeInBrstIdx);
    DFEVar iNumPkts  = ~ovflow_In ? constant.var(pktCntType, GC.PKTSPERBRST)
                                 : constant.var(pktCntType, 2*GC.PKTSPERBRST);
    SMIO ctrlSM = addStateMachine("NbrPropRdrCtrl", 
                          new QRdrPktCntSM(this, pktCntBits, GC.elemIdxRamDepth));
    ctrlSM.connectInput("pktVal",     nodeArrayCtrl);
    ctrlSM.connectInput("numPkts",    iNumPkts);
    ctrlSM.connectInput("numPktsVal", elemCtrl);
    DFEVar incRamRdPtr = ctrlSM.getOutput("incRdPtr");
    DFEVar pktCnt      = ctrlSM.getOutput("pktCnt");
    //accumulate the stall backpressure with all downstream stall signals
    //rising and falling of this signal controls when backpressure is propagated
    DFEVar inputStall  = ctrlSM.getOutput("stall") | outputStall;

    
    // Elem Num FIFO
    //   Stores queue of elems to use when processing incoming edge lists
    //   Pop the queue when all associated packets have been processed
    Count.Params ramRdPtrParams = control.count.makeParams(GC.eIdxRamDepthBits) //RdPtr
        .withEnable(incRamRdPtr);
    Count.Params ramWrPtrParams = control.count.makeParams(GC.eIdxRamDepthBits) //WrPtr
        .withEnable(elemCtrl);
    Counter ramRdPtrCounter = control.count.makeCounter(ramRdPtrParams);
    Counter ramWrPtrCounter = control.count.makeCounter(ramWrPtrParams);
    DFEVar ramRdPtr = ramRdPtrCounter.getCount();
    DFEVar ramWrPtr = ramWrPtrCounter.getCount();
    Memory <DFEVar> elemRam = mem.alloc( dfeUInt(32), GC.elemIdxRamDepth);
    elemRam.write(ramWrPtr, elem_In, elemCtrl);
    DFEVar elem    = elemRam.read(ramRdPtr);
    DFEVar elemIdx = elem.slice(0, GC.nPBBits).cast(dfeUInt(GC.nPBBits));


    // knlDone
    // Determine when the kernel has finished processing for this curLvl
    DFEVar elemIdxCtrl_d = stream.offset(elemCtrl, -1); //avoid blip with first ctrl
    DFEVar begun = Reductions.streamHold(elemIdxCtrl_d, elemIdxCtrl_d,
                                         dfeBool().encodeConstant(false));
    DFEVar emptyRam = ramRdPtr.eq(ramWrPtr);
    DFEVar knlDone = begun & emptyRam;
    DFEVar knlDonePrev = stream.offset(knlDone, -1);
    DFEVar doneVal = knlDone & knlDonePrev;
    Count.Params sendDoneParams = control.count.makeParams(32)
      .withEnable(doneVal);
    DFEVar sendDoneCnt = (control.count.makeCounter(sendDoneParams)).getCount();
    DFEVar sendLength = uDVal*2;
    DFEVar sendDone = knlDone & (sendDoneCnt < sendLength);
    
    DFEVar elemIdxP1  = elemIdx+1;
    DFEVar ovflow     = elemIdxP1.eq(0);
    DFEVar pktRawBits = constant.var(dfeUInt(buf), 0) # 
                        elemIdxP1.slice(GC.nPPBits, GC.pktsPBBits);
    DFEVar elemPkt = ~ovflow ? pktRawBits.cast(pktCntType)
                             : constant.var(pktCntType, GC.PKTSPERBRST);
    DFEVar thisPkt = elemPkt.eq(pktCnt); //request emitted during this pkt's cycle

    //DFEVar elemP1_pkt = (elemIdxP1.slice(GC.nPPBits, GC.pktsPBBits)).cast(dfeUInt(GC.pktsPBBits));
    DFEVar elem_mux_sel   = elemIdx.slice(0, GC.nPPBits);
    DFEVar elemP1_mux_sel = elemIdxP1.slice(0, GC.nPPBits);
    
    DFEVar newPkt  = elemP1_mux_sel.eq(0);
    DFEVar ptr0Array  = control.mux(elem_mux_sel, nodeArray.elementsAsArray());
    DFEVar edgePtr0   = ~newPkt ? ptr0Array : stream.offset(ptr0Array, -1);
    DFEVar edgePtr1P1 = control.mux(elemP1_mux_sel, nodeArray.elementsAsArray());
    DFEVar edgePtr1   = edgePtr1P1 - 1;

    DFEVar noNbrs     = edgePtr1P1.eq(edgePtr0); //account for the case where no nbrs
    
    //Possible Optimization:  Don't read multiple times if the node burst has
    //already been read.  Do the calculation and just use the array multiple
    //times instead of re-reading it.

    // Outputs to MemUnit: read node array
    //addr
    DFEType reqAddrType  = dfeUInt(GC.brstNbits);
    DFEType reqSizeType  = dfeUInt(GC.sizeBits);
    DFEVar nodeAddrBase  = nodeAddr.cast(reqAddrType);
    int nodeBrstBuf      = GC.nodeWidth - GC.nPBBits;
    int nodeBrstBuf2     = GC.brstNbits - nodeBrstBuf;
    DFEVar zeros2        = constant.var(dfeUInt(nodeBrstBuf2), 0);
    DFEVar nodeBrstRaw   = elem_In.slice(GC.nPBBits, nodeBrstBuf);  //request incoming node
    DFEVar nodeBrst      = (zeros2 # nodeBrstRaw).cast(reqAddrType);
    DFEVar req_brst_addr = nodeBrst+nodeAddrBase;
    //size
    DFEVar lastNodeInBrst = elemIdx_In.eq(lastNodeInBrstIdx);
    DFEVar req_size = lastNodeInBrst ?  constant.var(reqSizeType, 2) : 
                        constant.var(reqSizeType, 1);
    //enable
    DFEVar req_en    = elemCtrl;
    io.output("memReq", req_size#req_brst_addr#req_en, dfeRawBits(GC.memReqWidth));

    // Output to reducer
    DFEVar edgeBrst0  = edgePtr0.slice(GC.nPBBits, GC.brstNbits).cast(reqAddrType);
    DFEVar edgeBrst1  = edgePtr1.slice(GC.nPBBits, GC.brstNbits).cast(reqAddrType);
    DFEVar edgeBrsts  = edgeBrst1-edgeBrst0+1;
    DFEVar edgePkts   = edgeBrsts << GC.pktsPBBits;
    DFEVar edgePkts_  = edgePkts.slice(0, GC.numPktsBits);
    DFEVar edgePtr0_ = edgePtr0.slice(0, GC.ePtrWidth);
    DFEVar edgePtr1_ = edgePtr1.slice(0, GC.ePtrWidth);
    int ePtrDataWidth = 32 + GC.numPktsBits + GC.ePtrWidth + GC.ePtrWidth;
    DFEVar sendPtrs = (thisPkt & nodeArrayCtrl) & ~noNbrs;
    io.output("ptrData", elem#edgePkts_#edgePtr1_#edgePtr0_, 
              dfeRawBits(ePtrDataWidth), sendPtrs);

    // Output to MemUnit: read replicated array (for the reducer)
    DFEVar repAddrBase       = repAddr.cast(reqAddrType);
    DFEVar req_brst_addr_rep = repAddrBase + edgeBrst0;
    DFEVar req_size_rep      = edgeBrsts.cast(reqSizeType);
    DFEVar req_en_rep        = sendPtrs;
    io.output("memReqRep", req_size_rep#req_brst_addr_rep#req_en_rep,
              dfeRawBits(GC.memReqWidth));

    // Outputs to Done kernel
    io.output("idle", knlDone, dfeBool(), sendDone); 

    // Stall feedback:  flow control to previous unit
    DFEVar inputStallD = stream.offset(inputStall, -1);
    DFEVar startStall  = inputStall & ~inputStallD;
    DFEVar stopStall   = ~inputStall & inputStallD;
    io.output("stall", inputStall, dfeBool(), startStall|stopStall);

    /*
    /////////////////////////////////////////
    // Debug -- Scalar Outputs
    DFEVar sawIncRdPtr = Reductions.streamHold(incRamRdPtr, incRamRdPtr,
                                              dfeBool().encodeConstant(false));
    DFEVar sawBegun = Reductions.streamHold(begun, begun,
                                           dfeBool().encodeConstant(false));
    DFEVar sawArray = Reductions.streamHold(nodeArrayCtrl, nodeArrayCtrl,
                                           dfeBool().encodeConstant(false));
      //elemIdx counter
      Count.Params eCParams = control.count.makeParams(32)
        .withEnable(elemCtrl);
      Counter eCounter = control.count.makeCounter(eCParams);
      DFEVar eCnt = eCounter.getCount();

    io.scalarOutput("scalarOut_0", eCnt, dfeUInt(32));
    //io.scalarOutput("scalarOut_", sawBegun, dfeBool());
    //io.scalarOutput("scalarOut_", sawArray, dfeBool());
    */


    // debug
    debug.printf("NbrPropRdr: cnt=%d, pktCnt=%d, nodeAddr=%d, repAddr=%d, outputStall=%d, inputStall=%d\n", 
      cnt, pktCnt, nodeAddr, repAddr, outputStall, inputStall);
    debug.printf("    elemCtrl=%d, elem_In=%d, elemIdx_In=%d, wrPtr=%d, rdPtr=%d\n",
        elemCtrl, elem_In, elemIdx_In, ramWrPtr, ramRdPtr);
    debug.printf("    inArCtrl=%d, numPkts=%d, elem=%d, idx=%d, edgePtr0=%d, edgePtr1=%d, sendPtrs=%d\n",
				nodeArrayCtrl, edgePkts, elem, elemIdx, edgePtr0, edgePtr1, sendPtrs);
    debug.printf("    nodeArray = "); printArray(nodeArray.elementsAsArray(), arSize);
    debug.printf("\n");
    debug.printf("    [pkts] incRd=%d, ovflow=%d, hotPkt=%d, req_addr=%d, req_size=%d\n", 
       incRamRdPtr, ovflow, elemPkt, req_brst_addr, req_size);
    debug.printf("    thisPkt=%d, edgeBrst0=%d, edgeBrst1=%d, begun=%d, emptyRam=%d\n", 
      thisPkt, edgeBrst0, edgeBrst1, begun, emptyRam);
    debug.printf("    req_en_rep(sendPtrs)=%d, req_size_rep(edgeBrsts)=%d, req_addr_rep=%d\n",
      req_en_rep, req_size_rep, req_brst_addr_rep);
    debug.printf("    knlDone=%d, doneVal=%d, sendDoneCnt=%d, sendDone=%d\n",
        knlDone, doneVal, sendDoneCnt, sendDone);
    
    // a watch variable seems necessary for debug.printf to work
    cnt.watch("cnt"); 
    flush.onTrigger(constant.var(false));
  }
}
